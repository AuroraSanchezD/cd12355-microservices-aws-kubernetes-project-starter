1. Set up AWS profile with access key id and aws secret access key
2. Create EKS cluster (my-cluster) with its nodegroup (my-nodes) and set up the node type, how many nodes the cluster will have and min/max number of nodes. This cluster creation will trigger the formation of a CloudFormation stack. I am creating a Kubernetes cluster to manage my containerised application. Kubernetes will handle situations in which a container fails and another one needs to start.
3. Allowed CloudWatch logging in my newly created cluster by running:
     eksctl utils update-cluster-logging --enable-types=all --region=us-east-1 --cluster=my-cluster --approve
4. Updating the context in the local Kubeconfig file
5. Configurated the database for my coworking application by creating a PersistentVolumeClaim (PVC), a PersistentVolume (PV) and a Postgres deployment file (all these three are .yaml). PVCs and PVs manage storage. PVs represent the actual supply of storage that exists in the cluster while the PVC represents the demand for storage my application needs. The postgresql-deployment.yaml defines specs for containers, and the postgresql-service.yaml defines ports for the postgresql-service. Running these three files with kubectl -f will create the database deployment. Running postgresql-service.yaml will create the postgresql-service (ATTACH SCREENSHOT). Afterwards, it will be necessary to run the three sql files to create tables and fill them up with data to populate my database.
6. Created a Dockerfile inside of the analytics folder( ADD LINKS TO FILES AND FURTHER EXPLANATION). This Dockerfile will create a Docker image (test-coworking-analytics)
7. Created an ECR repo (ATTACH LIN) where we will be able to push the local test-coworking-analytics image.